\documentclass[11pt]{article}

\usepackage[letterpaper,left=1in,right=1in,top=1in,bottom=1in]{geometry}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{lmodern}
\usepackage{amssymb,amsmath}

\setcounter{secnumdepth}{0}

%\usepackage[vlined]{algorithm2e}
\usepackage{algorithm,algorithmic}
\usepackage{paralist}
\usepackage{tikz}
\usepackage{xcolor,colortbl}
\usepackage{multicol}

\usepackage{listings}

\begin{document}
\lstset{
language=Python,
basicstyle=\scriptsize,        % the size of the fonts that are used for the code
frame=single,                    % adds a frame around the code
keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
keywordstyle=\color{blue},
commentstyle=\color{gray}
}
\setlength{\parindent}{0in}
\addtolength{\parskip}{0.1cm}
\setlength{\fboxrule}{.5mm}\setlength{\fboxsep}{1.2mm}
\newlength{\boxlength}\setlength{\boxlength}{\textwidth}
\addtolength{\boxlength}{-4mm}

\begin{center}\framebox{\parbox{\boxlength}{{\bf
CS 4740, Spring 2014 \hfill Project 2 Report}\\
Anthony Chen (apc52)\\ 
James Feher (jkf49)\\ 
Matthew Oey (mso32)
}}
\end{center}

\section{Approach}
\subsection{Supervised WSD}
\subsubsection{Parsing Input Data}
To parse the input data, we 
\begin{lstlisting}
#wordmap is a mapping of word strings to word objects
def parse_train_data(filename):
    word_map = {}
    with open(filename) as f:
        for line in f:
            parts = re.split(' \| ', line)
            splitByPeriod = re.split('\.', parts[0])
            word = splitByPeriod[0]
            pos = splitByPeriod[1]
            sense = parts[1]
            context = parts[2]

            #Create word map
            if not word in word_map:
                word_map[word] = Word(word, pos)
            word_map[word].add_context(sense, context)

    return word_map
\end{lstlisting}


\begin{lstlisting}
def get_sense_count(word_map):
    word_to_sense_to_count_map = {}
    for word_object in word_map.values():
        sense_to_count_map = {}
        for sense, context_list in word_object.sense_id_map.items():
            sense_to_count_map[sense] = float(len(context_list))
        word_to_sense_to_count_map[word_object.word] = sense_to_count_map
    return word_to_sense_to_count_map
\end{lstlisting}


\subsection{Dictionary WSD}
\subsubsection{Parsing and Data Structures}
The dictionary datafile parser generates a Dictionary object, which is a Python dictionary that holds Word objects. Word objects were also used in the parsing of the supervised WSD datasets. The Word object was used in both WSD methods because there were overlaps in the test dataset parsing methods. The Word object was augmented, however, to contain a dictionary of senses. For every word, the dictionary dataset parser created its constituent senses as Sense objects. Figure~\ref{sense} shows the data structure. The gloss and example sentences were tokenized using NLTK.

\begin{figure}[ht]
\begin{lstlisting}
class Sense:
    def __init__(self,id,wordnet_ids,gloss,examples):
        self.id = id # The dictionary dataset sense of a word
        self.wordnet_ids = wordnet_ids # List of characters
        self.gloss = gloss # List of tokens (lemmatized or not)
        self.examples = examples # List of lists of tokens (lemmatized or not)
\end{lstlisting}
\caption{Sense object for dictionary WSD}
\label{sense}
\end{figure}

\subsubsection{Word-sense Disambiguation}
The WSD algorithm is similar to Lesk's algorithm. First, the method parses the test dataset using parse\_test\_data().

\subsubsection{Feature Selection}
IDF threshold
lol

\subsubsection{Overlap Calculation}
foo

\subsubsection{Lemmatization}
We included an optional lemmatization step in both the parsing of the dictionary dataset and the disambiguation. Tokens from the sense gloss and example sentences are lemmatized, as well as the features provided to word\_sense\_disambiguation() and the feature synset definition and example sentences. The reasoning in lemmatizing tokens was to reduce false negatives in calculating overlaps, due to morphological variations in words.


\section{Software}
\subsection{Supervised WSD}
For Supervised WSD we used NLTK 

\subsection{Dictionary WSD}
The Python Standard Library ElementTree XML API was utilized to parse the dictionary XML file and generate a hierarchical set of data structures representing the dictionary datafile contents. Libraries from the Natural Language Toolkit (NLTK) were also used extensively in the parsing of the dictionary dataset and in the disambiguation process. Specifically, the word tokenizer, part of speech tagger, and lemmatizer were used.

\section{Results}
\subsection{Supervised WSD}
TODO

\subsection{Dictionary WSD}
TODO

\section{Discussion}
\subsection{Supervised WSD}
TODO

\subsection{Dictionary WSD}
TODO

\end{document}
